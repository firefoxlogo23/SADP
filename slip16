import numpy as np
# Set random seed for reproducibility
np.random.seed(42)
# Input data: 3 examples with 2 features each
X = np.array([[0.1, 0.2, 0.3],
              [0.4, 0.5, 0.6]])
# Network architecture
n_input = 2   # Number of input features
n_hidden = 4  # Number of neurons in the hidden layer
n_output = 1  # Output size (binary classification)
# Initialize weights and biases
W1 = np.random.randn(n_hidden, n_input) * 0.01  
b1 = np.zeros((n_hidden, 1))                    
W2 = np.random.randn(n_output, n_hidden) * 0.01 
b2 = np.zeros((n_output, 1))                    
# Forward propagation (without separate functions)
# Layer 1: Hidden Layer
Z1 = np.dot(W1, X) + b1  # Linear transformation
A1 = np.maximum(0, Z1)   # ReLU activation
# Layer 2: Output Layer
Z2 = np.dot(W2, A1) + b2  # Linear transformation
A2 = 1 / (1 + np.exp(-Z2))  # Sigmoid activation
# Output
print("Network Output:\n", A2)



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error

data=pd.read_csv("house.csv")
print(data)
data=data.dropna(axis=0,how=”any”)
print(data)

x=data[["sqft_living"]]
y=data.price

print(x)
print(y)

plt.scatter(x,y)
plt.xlabel("sqft_living")
plt.ylabel("price")
plt.show()

xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.2)
print(xtrain)
print(xtest)
print(ytrain)
print(ytest)

lr=LinearRegression()
lr.fit(xtrain,ytrain)

print(lr.intercept_)
print(lr.coef_)

print(lr.predict([[1000]]))


ypred=lr.predict(xtest)
cm=mean_absolute_error(ytest,ypred)
print(cm)
